# NOTES #

notes and important links for alfatraining course in deep learning, in 2023  
(version: 02:12 07.03.2024)  

<base target="_blank">


Deep Learning Book: A. Geron, 2023, "Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und TensorFlow"  

* [Machine Learning Book web](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)  
* [Machine Learning Book](https://homl.info/er3)  
* [Codes from the book in github](https://github.com/ageron/handson-ml3)  


Neural Networks videos from 3Blue 1Brown:  

* [Neural Network youtube playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)  


First day discussion links:  

* [TensorFlow and Differential Equations](https://medium.com/@fjpantunes2/tensorflow-and-differential-equations-a-simple-example-77d88d98ea3e), on medium  
* [Physics-informed neural networks](https://en.wikipedia.org/wiki/Physics-informed_neural_networks), on wikipedia  
* [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), on wikipedia  


Neural Network, from previous machine learning course:  

* [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901), on arXiv  
* [Your Deep Learning Journey](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb), from fastai book, on github  
* [The fastai book](https://github.com/fastai/fastbook), on github, using PyTorch  
* [Geometric Intuition for Training Neural Networks](https://www.youtube.com/watch?v=Z_MA8CWKxFU&t=0s), on youtube (31 minutes)   
* [Watching Neural Networks Learn](https://www.youtube.com/watch?v=TkwXa7Cvfr8&t=0s), on youtube (26 minutes)  
* [other machine learning notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Machine_Learning/my_notes/notes.md)  
* [other machine learning notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_machine_learning/src/master/my_notes/notes.md)  


[![Geometric Intuition for Training Neural Networks](https://img.youtube.com/vi/Z_MA8CWKxFU/0.jpg)](https://www.youtube.com/watch?v=Z_MA8CWKxFU&t=0s)


[![Watching Neural Networks Learn](https://img.youtube.com/vi/TkwXa7Cvfr8/0.jpg)](https://www.youtube.com/watch?v=TkwXa7Cvfr8&t=0s)


Encoding:  

* [One-hot encoding](https://en.wikipedia.org/wiki/One-hot), on wikipedia  
* [1-aus-n-Code](https://de.wikipedia.org/wiki/1-aus-n-Code), on german wikipedia  


Rangfolge:  

* [Ranking](https://en.wikipedia.org/wiki/Ranking), on wikipedia  
* [Rangordnung](https://de.wikipedia.org/wiki/Rangordnung), on german wikipedia  


TensorFlow:  

* [TensorFlow website](https://www.tensorflow.org/)  
* [TensorFlow installation with Anaconda](https://docs.anaconda.com/free/anaconda/applications/tensorflow/)  
* [TensorFlow installation with pip](https://www.tensorflow.org/install/pip#windows-native)  
* [TensorFlow](https://en.wikipedia.org/wiki/TensorFlow), on wikipedia  
* [TensorFlow](https://de.wikipedia.org/wiki/TensorFlow), on german wikipedia  
* [TensorFlow](https://id.wikipedia.org/wiki/TensorFlow), di wikipedia Indonesia  


Keras:  

* [Keras website](https://keras.io/)  
* [Keras API](https://keras.io/api/)  
* [Keras](https://en.wikipedia.org/wiki/Keras), on wikipedia  
* [Keras](https://de.wikipedia.org/wiki/Keras), on german wikipedia  


PyTorch:

* [PyTorch website](https://pytorch.org/)  
* [PyTorch](https://en.wikipedia.org/wiki/PyTorch), on wikipedia  
* [PyTorch](https://de.wikipedia.org/wiki/PyTorch), on german wikipedia  
* [PyTorch](https://id.wikipedia.org/wiki/PyTorch), di wikipedia Indonesia  


OpenCV-Python

* [OpenCV-Python Tutorials](https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html)  
* [opencv-python, on pypi](https://pypi.org/project/opencv-python/), as wrapper package for OpenCV python bindings  


OpenNN:  open-source neural networks library for machine learning with C++  

* [OpenNN website](https://www.opennn.net/)  
* [OpenNN on github](https://github.com/Artelnics/opennn)  
* [OpenNN](https://en.wikipedia.org/wiki/OpenNN), on wikipedia  
* [OpenNN](https://de.wikipedia.org/wiki/OpenNN), on german wikipedia  


Apache Mahout:  distributed linear algebra framework and mathematically expressive Scala DSL  

* [Apache Mahout website](https://mahout.apache.org/)  
* [Apache Mahout](https://en.wikipedia.org/wiki/Apache_Mahout), on wikipedia  


Google JAX:  machine learning framework for transforming numerical functions  

* [JAX: Autograd and XLA](https://github.com/google/jax), on github  
* [Google JAX](https://en.wikipedia.org/wiki/Google_JAX), on wikipedia  


MATLAB & Octave:  

* [MATLAB](https://www.mathworks.com/products/matlab.html)  
* [Octave](https://octave.org/)  


first week coding tips:  

* [numpy.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#numpy.reshape)  
* ["cloning" a row or column vector to a matrix](https://stackoverflow.com/questions/1550130/cloning-row-or-column-vectors)  
* [Error in Python script "Expected 2D array, got 1D array instead:"](https://stackoverflow.com/questions/45554008/error-in-python-script-expected-2d-array-got-1d-array-instead)  
* [Reading tab-delimited file with Pandas](https://stackoverflow.com/questions/27896214/reading-tab-delimited-file-with-pandas-works-on-windows-but-not-on-mac)  
* [Load a pandas DataFrame, with TensorFlow](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)  


A Neural Network PlayGround: Tinker With a Neural Network:  

* [TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.83868&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)  
* [TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=10&networkShape=7,5,3&seed=0.83001&showTestData=false&discretize=false&percTrainData=60&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false)  


loss function & minimization/optimization:  

* [Least squares](https://en.wikipedia.org/wiki/Least_squares), on wikipedia  
* [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate), on german wikipedia  


Activation functions:  

* [Layer activation functions, in Keras](https://keras.io/api/layers/activations/)  
* [Activation function](https://en.wikipedia.org/wiki/Activation_function), on wikipedia  
* [Aktivierungsfunktionen von Künstliches Neuron](https://de.wikipedia.org/wiki/K%C3%BCnstliches_Neuron#Aktivierungsfunktionen), on german wikipedia  
* [plot of Logistic function](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py)  


third day discussion links:  

* [Why Gaussian Error Linear Units (GELUs) activation function is used instead of ReLu in BERT?](https://stackoverflow.com/questions/57532679/why-gelu-activation-function-is-used-instead-of-relu-in-bert)  
* [Levenberg–Marquardt algorithm](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm), on wikipedia  
* [Levenberg-Marquardt-Algorithmus](https://de.wikipedia.org/wiki/Levenberg-Marquardt-Algorithmus), on german wikipedia  
* [Glorot and Bengio, 2010](https://proceedings.mlr.press/v9/glorot10a.html) paper, "Understanding the difficulty of training deep feedforward neural networks"  
* [Enable GPU acceleration for TensorFlow 2 with tensorflow-directml-plugin](https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin)  

```
pip install tensorflow-directml-plugin  
```


Deep Learning: Neuronale Netze mit Keras  

* [Dense layer, in Keras](https://keras.io/api/layers/core_layers/dense/)  
* [Keras - Dense Layer](https://www.tutorialspoint.com/keras/keras_dense_layer.htm) tutorial  
* [Model training APIs, in Keras](https://keras.io/api/models/model_training_apis/)  
* [Create a Machine Learning model with Keras and TensorFlow](https://towardsdatascience.com/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3):  Sequential, Functional, and Model Subclassing  


Keras Model:  

* [Models API, in Keras](https://keras.io/api/models/)  
* [The Model class, in Keras](https://keras.io/api/models/model/#model-class)  
* [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model), in TensorFlow documentation  


Keras Model evaluation:  

* [evaluate in tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate), in TensorFlow documentation  
* [How to evaluate a keras model?](https://www.projectpro.io/recipes/evaluate-keras-model)  
* [Keras - Model Evaluation and Model Prediction](https://www.tutorialspoint.com/keras/keras_model_evaluation_and_prediction.htm) tutorial  


Keras callbacks:  

* [Callbacks API, in Keras](https://keras.io/api/callbacks/)  
* A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)  
* [LambdaCallback class, in Keras](https://keras.io/api/callbacks/lambda_callback/), as the Callback for creating simple, custom callbacks on-the-fly  
* [RemoteMonitor class, in Keras](https://keras.io/api/callbacks/remote_monitor/), as the Callback used to stream events to a server  
* [tf.keras.callbacks.RemoteMonitor](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/RemoteMonitor), in TensorFlow documentation  


Examples of data, in scikit-learn (sklearn), Keras, etc:  

* [Real world datasets, in sklearn](https://scikit-learn.org/stable/datasets/real_world.html)  
* [California Housing price regression dataset, in Keras](https://keras.io/api/datasets/california_housing/)  
* [fetch_california_housing, in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), to load the California housing dataset  
* [astroNN](https://astronn.readthedocs.io/en/stable/) is a python package to do various kinds of neural networks with targeted application in astronomy by using Keras API  
* [Galaxy10 SDSS Dataset, in astroNN](https://astronn.readthedocs.io/en/latest/galaxy10sdss.html) 
* [Download Galaxy10 SDSS](https://astronn.readthedocs.io/en/latest/galaxy10sdss.html#download-galaxy10-sdss) 


Metrics, in scikit-learn (sklearn) and Keras:  

* list of [Metrics, in Keras API](https://keras.io/api/metrics/), contain functions that are used to judge the performance of your model  
* list of [Regression metrics, in Keras](https://keras.io/api/metrics/regression_metrics/), contain classes: MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredLogarithmicError, CosineSimilarity, LogCoshError  
* [tf.keras.metrics.R2Score](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/R2Score), in TensorFlow documentation  
* [r2_score, in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score), for computing the coefficient of determination  


Loss Function, in Keras:  

* list of [Losses, in Keras API](https://keras.io/api/losses/)  
* list of [Probabilistic losses, in Keras API](https://keras.io/api/losses/probabilistic_losses/)  
* list of [Regression losses, in Keras API](https://keras.io/api/losses/regression_losses/)  


Cross Entropy:  

* [Cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy), on wikipedia  
* [Kreuzentropie](https://de.wikipedia.org/wiki/Kreuzentropie), on german wikipedia  
* [One-Hot Encoding for Machine Learning with TensorFlow 2.0 and Keras](https://github.com/christianversloot/machine-learning-articles/blob/main/one-hot-encoding-for-machine-learning-with-tensorflow-and-keras.md), on github  


Calibration curve / Lernkurven, in scikit-learn (sklearn):  

* [Probability calibration, in sklearn](https://scikit-learn.org/stable/modules/calibration.html)  


Permutation feature importance, in scikit-learn (sklearn):  

* [Permutation feature importance, in sklearn](https://scikit-learn.org/stable/modules/permutation_importance.html)  
* [permutation_importance, in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html), as Permutation importance for feature evaluation  


fourth day discussion links:  

* [Image Classification on CIFAR-10](https://paperswithcode.com/sota/image-classification-on-cifar-10), for benchmarking  
* [Gemini, from Google DeepMind](https://deepmind.google/technologies/gemini/)  
* [Hands-on with Gemini](https://deepmind.google/technologies/gemini/#hands-on)  
* [Andrew Ng Criticizes the Culture of Overfitting in Machine Learning](https://www.unite.ai/andrew-ng-criticizes-the-culture-of-overfitting-in-machine-learning/)  


Handling Overfitting:  

* [Layer weight regularizers, in Keras](https://keras.io/api/layers/regularizers/), for implementing L1 & L2 Regularization  
* [tf.keras.regularizers.Regularizer](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer), in TensorFlow documentation  
* [Dropout layer, in Keras](https://keras.io/api/layers/regularization_layers/dropout/)  
* [EarlyStopping, in Keras](https://keras.io/api/callbacks/early_stopping/)  

```
pip install keras-tuner  
```


Optimization/Optimierung Übung/Exercise: Drop Out  

* [Drop Out, slides](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Deep_Learning/Woche_1/Drop_out.pdf), in PDF, on github  
* [Drop Out, slides](https://bitbucket.org/iscab/alfatraining_2023_deep_learning/src/master/Woche_1/Drop_out.pdf), in PDF, on bitbucket  
* google docs link?  


fifth day discussion links:  

* [Never use restore_best_weights=True with EarlyStopping](https://medium.com/@doleron/never-use-restore-best-weights-true-with-earlystopping-754ba5f9b0c6)  
* [hyperbolicfitdll, on github](https://github.com/bschulz81/hyperbolicfitdll), as an open source library that can help to autofocus telescopes  
* [robustregression, on github](https://github.com/bschulz81/robustregression), as a c++ library with statistical machine learning algorithms for linear and non-linear robust regression  


Python quiz, for LinkedIn:  

* [Python quiz](https://www.w3schools.com/python/python_quiz.asp), on w3schools  


Gradient Descent: Vanilla, Stochastic, and Mini Batch  

* [Differences Between Gradient, Stochastic and Mini Batch Gradient Descent](https://www.baeldung.com/cs/gradient-stochastic-and-mini-batch)  
* [Difference between Batch Gradient Descent and Stochastic Gradient Descent](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/)  
* [Difference Between SGD, GD, and Mini-batch GD](https://www.tutorialspoint.com/difference-between-sgd-gd-and-mini-batch-gd)  


Tensorboard  

* [TensorBoard: TensorFlow's visualization toolkit](https://www.tensorflow.org/tensorboard)  
* [Get started with TensorBoard](https://www.tensorflow.org/tensorboard/get_started)  
* ['tensorboard' is not recognized as an internal or external command,](https://stackoverflow.com/questions/47985835/tensorboard-is-not-recognized-as-an-internal-or-external-command), on stackoverflow  
* check Tensorboard results on [http://localhost:6006](http://localhost:6006)  

```
tensorboard --logdir=./my_logs --port=6006  
```

OR

```
python -m tensorboard.main --logdir=./my_logs --port=6006  
```


Graphviz:  

* [Graphviz, on gitlab](https://graphviz.gitlab.io/), as open source graph visualization software, for representing structural information as diagrams of abstract graphs and networks  
* [Graphviz download](https://graphviz.gitlab.io/download/)  
* [Graphviz website](https://graphviz.org/)  
* [Graphviz download](https://graphviz.org/download/)  
* [Graphviz](https://en.wikipedia.org/wiki/Graphviz), on wikipedia  
* [Graphviz](https://de.wikipedia.org/wiki/Graphviz), on german wikipedia  


pydot: Python interface to Graphviz's Dot  


* [pydot](https://github.com/pydot/pydot), on github  
* pydot is is an interface to Graphviz  
* pydot can parse and dump into the DOT language used by GraphViz  
* pydot is written in pure Python
* [pydot 1.2.2, on pypi](https://pypi.org/project/pydot/1.2.2/)  
* [pydot](https://anaconda.org/anaconda/pydot), for anaconda  

```
pip install pydot==1.2.2  
```

OR

```
conda install anaconda::pydot  
```


sixth day discussion links:  

* [AI Act: Einigung auf Regeln für künstliche Intelligenz](https://www.sueddeutsche.de/wirtschaft/ai-act-kuenstliche-intelligenz-bruessel-ki-1.6316909), on Süddeutsche Zeitung  


Convolutional Neural Network (CNN):  

* [2D convolution layer, in Keras](https://keras.io/api/layers/convolution_layers/convolution2d/)  
* [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D), in TensorFlow documentation  
* list of [Convolution layers, in Keras](https://keras.io/api/layers/convolution_layers/)  
* list of [Pooling layers, in Keras](https://keras.io/api/layers/pooling_layers/)  
* [MaxPooling2D layer, in Keras](https://keras.io/api/layers/pooling_layers/max_pooling2d/)  
* [tf.keras.layers.MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D), in TensorFlow documentation  
* [MaxPooling2D layer, in Keras 2.15](https://keras.io/2.15/api/layers/pooling_layers/max_pooling2d/)  
* [GlobalMaxPooling2D layer, in Keras](https://keras.io/api/layers/pooling_layers/global_max_pooling2d/)  
* [tf.keras.layers.GlobalMaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPooling2D), in TensorFlow documentation  
* list of [Reshaping layers, in Keras](https://keras.io/api/layers/reshaping_layers/)  
* [Flatten layer](https://keras.io/api/layers/reshaping_layers/flatten/)  
* [tf.keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten), in TensorFlow documentation  
* [Keras Applications](https://keras.io/api/applications/), as deep learning models that are made available alongside pre-trained weights, for prediction, feature extraction, and fine-tuning  
* [2D Visualization of Convolutional Neural Network](https://adamharley.com/nn_vis/cnn/2d.html), with examples of hand-written digits recognition  
* [load_digits, in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html), to Load and return the UCI ML hand-written digits dataset (classification)  
* [Understanding Convolutional Neural Network (CNN)](https://learnopencv.com/understanding-convolutional-neural-networks-cnn/): A Complete Guide, with VGG-16 as an example  


CNN Task: LFW datasets (Labeled Faces in the Wild)  

* [Labeled Faces in the Wild (LFW)](https://datagen.tech/guides/image-datasets/lfw-dataset/), as an image dataset containing face photographs, collected especially for studying the problem of unconstrained face recognition  


seventh day discussion links:  

* [Understanding Padding in Machine Learning](https://deepai.org/machine-learning-glossary-and-terms/padding)  
* [Deep Neural Networks: Padding](https://www.baeldung.com/cs/deep-neural-networks-padding)  
* [Image Classification on ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet), for benchmarking  


CNN Übung/Exercise: GoogleNet  

* inception layer  
* google docs link?  
* [GoogLeNet](https://paperswithcode.com/method/googlenet)  
* [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842v1), on arXiv  
* [Going Deeper with Convolutions, in PDF](https://arxiv.org/pdf/1409.4842v1.pdf), on arXiv  
* [GoogleNet implementation, in PyTorch](https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/googlenet.py#L62), on github  
* [A Simple Guide to the Versions of the Inception Network](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)  


Local Response Norm:  

* list of [Normalization layers, in Keras](https://keras.io/api/layers/normalization_layers/)  
* [tf.nn.local_response_normalization](https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization), in TensorFlow documentation  
* [What is local response normalization?](https://www.quora.com/What-is-local-response-normalization/answer/Hu-Yixuan)  


AlexNet:  

* [AlexNet](https://en.wikipedia.org/wiki/AlexNet), on wikipedia  
* AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto.  
* [AlexNet](https://paperswithcode.com/method/alexnet) is a classic convolutional neural network architecture, consisting of convolutions, max pooling and dense layers as the basic building blocks.  
* [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)  
* [ImageNet Classification with Deep Convolutional Neural Networks, in PDF](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)  
* [AlexNet, in PyTorch](https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40), on github  
* [AlexNet - CNN Explained and Implemented](https://www.youtube.com/watch?v=8GheVe2UmUM&t=0s), on youtube (13 minutes)  
* [Sensory illusions and lateral inhibition](https://braininbrief.tumblr.com/post/7975037341/sensory-illusions-and-lateral-inhibition), from Brain Never Sleeps  


[![AlexNet - CNN Explained and Implemented](https://img.youtube.com/vi/8GheVe2UmUM/0.jpg)](https://www.youtube.com/watch?v=8GheVe2UmUM&t=0s)


ResNet: Residual Network  

* [ResNet and ResNetV2, in Keras](https://keras.io/api/applications/resnet/)  
* [Residual neural network](https://en.wikipedia.org/wiki/Residual_neural_network), on wikipedia  
* [Residual Network](https://paperswithcode.com/method/resnet)  
* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385v1), on arXiv  
* [Deep Residual Learning for Image Recognition, in PDF](https://arxiv.org/pdf/1512.03385v1.pdf), on arXiv  
* with Batch Normalization  
* [a residual network using Keras' Sequential() API training on CIFAR10](https://gist.github.com/FirefoxMetzger/6b6ccf4f7c344459507e73bbd13ec541), on github  


Xception: Deep Learning with Depthwise Separable Convolutions  

* Depthwise Separable Convolutional Layer  
* [Xception, in Keras](https://keras.io/api/applications/xception/)  
* [tf.keras.applications.xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception), in TensorFlow documentation  
* [Xception](https://paperswithcode.com/method/xception)  
* [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357), on arXiv  
* [Xception: Deep Learning with Depthwise Separable Convolutions, in PDF](https://arxiv.org/pdf/1610.02357.pdf), on arXiv  
* [Xception, using Keras](https://github.com/keras-team/keras-applications/blob/bc89834ed36935ab4a4994446e34ff81c0d8e1b7/keras_applications/xception.py#L40), on github  
* [Xception: Deep Learning with Depthwise Separable Convolutions](https://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html)  
* [Xception: Deep Learning with Depthwise Separable Convolutions, in PDF](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf)  


Depthwise Separable Convolution:  

* [Depthwise Separable Convolution](https://paperswithcode.com/method/depthwise-separable-convolution)  
* [Xception, using TensorFlow](https://github.com/kwotsin/TensorFlow-Xception/blob/c42ad8cab40733f9150711be3537243278612b22/xception.py#L67), on github  


Depthwise Convolution:  

* [Depthwise Convolution is All You Need for Learning Multiple Visual Domains](https://paperswithcode.com/paper/depthwise-convolution-is-all-you-need-for)  
* [Depthwise Convolution is All You Need for Learning Multiple Visual Domains](https://arxiv.org/abs/1902.00927v2), on arXiv  
* [Depthwise Convolution is All You Need for Learning Multiple Visual Domains, in PDF](https://arxiv.org/pdf/1902.00927v2.pdf), on arXiv  
* [Depthwise_Convolution_for_Multiple_Domain_Learning](https://github.com/yunhuiguo/Depthwise_Convolution_for_Multiple_Domain_Learning), on github  


VGG:  

* [VGG16 and VGG19, in Keras](https://keras.io/api/applications/vgg/)  
* [tf.keras.applications.vgg16.preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input), in TensorFlow documentation  
* [How to calculate the number of parameters of convolutional neural networks?](https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networks)  
* [VGG](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/config3.jpg), on github  


MobileNet:  

* [MobileNet, MobileNetV2, and MobileNetV3, in Keras](https://keras.io/api/applications/mobilenet/)  
* [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861), on arXiv  
* [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, in PDF](https://arxiv.org/pdf/1704.04861.pdf), on arXiv  


Vision-Transformer (ViT)  

* [Vision transformer](https://en.wikipedia.org/wiki/Vision_transformer), on wikipedia  
* [Vision Transformer](https://paperswithcode.com/method/vision-transformer)  
* The Vision Transformer, or ViT, is a model for image classification that employs a Transformer-like architecture over patches of the image.  
* An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder.  
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929), on arXiv  
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, in PDF](https://arxiv.org/pdf/2010.11929.pdf), on arXiv  
* [Vision Transformer and MLP-Mixer Architectures](https://github.com/google-research/vision_transformer), on github  


Batch Normalization  

* [BatchNormalization layer, in Keras](https://keras.io/api/layers/normalization_layers/batch_normalization/)  
* [Batch normalization](https://en.wikipedia.org/wiki/Batch_normalization), on wikipedia  
* [What is Batch Normalization?](https://databasecamp.de/en/ml/batch-normalization-en)  
* [Batch Norm, Explained Visually: How it works, and why neural networks need it](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)    


eighth day discussion links

* [Models, in Kaggle](https://www.kaggle.com/models?tfhub-redirect=true), to search and discover hundreds of trained, ready-to-deploy machine learning models in one place.   
* [Compare Image Recognition Software](https://www.g2.com/categories/image-recognition)  
+ to be continued  


### Project: EEG Data processing and classification (CANCELLED)  

This topic were not chosen because the data loading was tricky and also my data was credential. 


Useful links for project:  

* [MNE in Python](https://github.com/mne-tools/mne-python), for MEG and EEG analysis and visualization  
* [MNE website](https://mne.tools/stable/index.html#)  
* [MNE dev](https://mne.tools/dev/index.html)  
* [MNE dev installation guide](https://mne.tools/dev/install/manual_install.html#manual-install)  
* [MNE installation guide](https://mne.tools/0.23/install/index.html)  
* [example from my MATLAB/EEGLAB code](https://bitbucket.org/iscab/bci_naovibe_footrace/src/master/EEG-analysis-script/scripts/ana01_filt_coaNAO.m)  


### Project: Image Super Resolution  

This topic is chosen because of data availability. 

Hints:  

* Super-Resolution  
* ResNet: Residual Network  
* Skip-connection  
* Batch Normalization  
* Peak signal-to-noise ratio (PSNR)  
* Structural similarity index measure (SSIM)  
* Image Quality metrics  
* Matplotlib  


drizzle algorithm:  

* suggestion "in astronomy one commonly uses a drizzle algorithm, perhaps you want to put that in as some layer?"  
* [Linear Reconstruction of the Hubble Deep Field](https://www.stsci.edu/~fruchter/dither/drizzle.html), explaining drizzle algorithm  
* suggestion "i suspect a working approach would be to apply drizzle 4x to enlarge the picture linearly and then apply a neuronal network, perhaps with convolutions in otder sharpen the image and make it smaller. the result is then a 2x larger image which is sharp"  


check:  

* [alfatraining](https://www.alfatraining.de/gefoerderte-weiterbildung/) courses  
* [alfatraining](https://www.alfatraining.de/) website  
* detailed in [my private repository](https://bitbucket.org/iscab/alfatraining_2023_deep_learning/)  
* [this deep learning notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Deep_Learning/my_notes/notes.md)  
* [this deep learning notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_deep_learning/src/master/my_notes/notes.md)  
* [other machine learning notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Machine_Learning/my_notes/notes.md)  
* [other machine learning notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_machine_learning/src/master/my_notes/notes.md)  
* [other python notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Python_Programmierung/my_notes/notes.md)  
* [other python notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_python/src/master/my_notes/notes.md)  
* [Learn Markdown](https://bitbucket.org/tutorials/markdowndemo)  


version: 02:12 07.03.2024

# End of File


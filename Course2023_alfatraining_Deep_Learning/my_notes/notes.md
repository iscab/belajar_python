# NOTES #

notes and important links for alfatraining course in deep learning, in 2023  
(version: 08:00 09.02.2024)  


Deep Learning Book: A. Geron, 2023, "Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und TensorFlow"  

* [Machine Learning Book web](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)  
* [Machine Learning Book](https://homl.info/er3)  
* [Codes from the book in github](https://github.com/ageron/handson-ml3)  


Neural Networks videos from 3Blue 1Brown:  

* [Neural Network youtube playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)  


First day discussion links:  

* [TensorFlow and Differential Equations](https://medium.com/@fjpantunes2/tensorflow-and-differential-equations-a-simple-example-77d88d98ea3e), on medium  
* [Physics-informed neural networks](https://en.wikipedia.org/wiki/Physics-informed_neural_networks), on wikipedia  
* [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), on wikipedia  


Encoding:  

* [One-hot encoding](https://en.wikipedia.org/wiki/One-hot), on wikipedia  
* [1-aus-n-Code](https://de.wikipedia.org/wiki/1-aus-n-Code), on german wikipedia  


Rangfolge:  

* [Ranking](https://en.wikipedia.org/wiki/Ranking), on wikipedia  
* [Rangordnung](https://de.wikipedia.org/wiki/Rangordnung), on german wikipedia  


TensorFlow:  

* [TensorFlow website](https://www.tensorflow.org/)  
* [TensorFlow installation with Anaconda](https://docs.anaconda.com/free/anaconda/applications/tensorflow/)  
* [TensorFlow installation with pip](https://www.tensorflow.org/install/pip#windows-native)  
* [TensorFlow](https://en.wikipedia.org/wiki/TensorFlow), on wikipedia  
* [TensorFlow](https://de.wikipedia.org/wiki/TensorFlow), on german wikipedia  
* [TensorFlow](https://id.wikipedia.org/wiki/TensorFlow), di wikipedia Indonesia  


Keras:  

* [Keras website](https://keras.io/)  
* [Keras API](https://keras.io/api/)  
* [Keras](https://en.wikipedia.org/wiki/Keras), on wikipedia  
* [Keras](https://de.wikipedia.org/wiki/Keras), on german wikipedia  


PyTorch:

* [PyTorch website](https://pytorch.org/)  
* [PyTorch](https://en.wikipedia.org/wiki/PyTorch), on wikipedia  
* [PyTorch](https://de.wikipedia.org/wiki/PyTorch), on german wikipedia  
* [PyTorch](https://id.wikipedia.org/wiki/PyTorch), di wikipedia Indonesia  


OpenCV-Python

* [OpenCV-Python Tutorials](https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html)  
* [opencv-python on pypi](https://pypi.org/project/opencv-python/) as wrapper package for OpenCV python bindings  


OpenNN:  open-source neural networks library for machine learning with C++  

* [OpenNN website](https://www.opennn.net/)  
* [OpenNN on github](https://github.com/Artelnics/opennn)  
* [OpenNN](https://en.wikipedia.org/wiki/OpenNN), on wikipedia  
* [OpenNN](https://de.wikipedia.org/wiki/OpenNN), on german wikipedia  


Apache Mahout:  distributed linear algebra framework and mathematically expressive Scala DSL  

* [Apache Mahout website](https://mahout.apache.org/)  
* [Apache Mahout](https://en.wikipedia.org/wiki/Apache_Mahout), on wikipedia  


Google JAX:  machine learning framework for transforming numerical functions  

* [Google JAX on github](https://github.com/google/jax)  
* [Google JAX](https://en.wikipedia.org/wiki/Google_JAX), on wikipedia  


MATLAB & Octave:  

* [MATLAB](https://www.mathworks.com/products/matlab.html)  
* [Octave](https://octave.org/)  


first week coding tips:  

* [numpy.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#numpy.reshape)  
* ["cloning" a row or column vector to a matrix](https://stackoverflow.com/questions/1550130/cloning-row-or-column-vectors)  
* [Error in Python script "Expected 2D array, got 1D array instead:"](https://stackoverflow.com/questions/45554008/error-in-python-script-expected-2d-array-got-1d-array-instead)  
* [Reading tab-delimited file with Pandas](https://stackoverflow.com/questions/27896214/reading-tab-delimited-file-with-pandas-works-on-windows-but-not-on-mac)  
* [Load a pandas DataFrame, with TensorFlow](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)  


A Neural Network PlayGround: Tinker With a Neural Network:  

* [TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.83868&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)  
* [TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=10&networkShape=7,5,3&seed=0.83001&showTestData=false&discretize=false&percTrainData=60&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false)  


loss function & minimization/optimization:  

* [Least squares](https://en.wikipedia.org/wiki/Least_squares), on wikipedia  
* [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate), on german wikipedia  


Activation functions:  

* [Layer activation functions, in Keras](https://keras.io/api/layers/activations/)  
* [Activation function](https://en.wikipedia.org/wiki/Activation_function), on wikipedia  
* [Aktivierungsfunktionen von Künstliches Neuron](https://de.wikipedia.org/wiki/K%C3%BCnstliches_Neuron#Aktivierungsfunktionen), on german wikipedia  
* [plot of Logistic function](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py)  


third day discussion links:  

* [Why Gaussian Error Linear Units (GELUs) activation function is used instead of ReLu in BERT?](https://stackoverflow.com/questions/57532679/why-gelu-activation-function-is-used-instead-of-relu-in-bert)  
* [Levenberg–Marquardt algorithm](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm), on wikipedia  
* [Levenberg-Marquardt-Algorithmus](https://de.wikipedia.org/wiki/Levenberg-Marquardt-Algorithmus), on german wikipedia  
* [Glorot and Bengio, 2010](https://proceedings.mlr.press/v9/glorot10a.html) paper, "Understanding the difficulty of training deep feedforward neural networks"  
* [Enable GPU acceleration for TensorFlow 2 with tensorflow-directml-plugin](https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin)  

```
pip install tensorflow-directml-plugin  
```


Deep Learning: Neuronale Netze mit Keras  

* [Dense layer, in Keras](https://keras.io/api/layers/core_layers/dense/)  
* [Keras - Dense Layer](https://www.tutorialspoint.com/keras/keras_dense_layer.htm) tutorial  
* [Model training APIs, in Keras](https://keras.io/api/models/model_training_apis/)  
* [Create a Machine Learning model with Keras and TensorFlow](https://towardsdatascience.com/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3):  Sequential, Functional, and Model Subclassing  


Keras Model:  

* [Models API, in Keras](https://keras.io/api/models/)  
* [The Model class, in Keras](https://keras.io/api/models/model/#model-class)  
* [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model), in TensorFlow documentation  


Keras Model evaluation:  

* [evaluate in tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate), in TensorFlow documentation  
* [How to evaluate a keras model?](https://www.projectpro.io/recipes/evaluate-keras-model)  
* [Keras - Model Evaluation and Model Prediction](https://www.tutorialspoint.com/keras/keras_model_evaluation_and_prediction.htm) tutorial  


Keras callbacks:  

* [Callbacks API, in Keras](https://keras.io/api/callbacks/)  
* A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)  
* [LambdaCallback class, in Keras](https://keras.io/api/callbacks/lambda_callback/), as the Callback for creating simple, custom callbacks on-the-fly  
* [RemoteMonitor class, in Keras](https://keras.io/api/callbacks/remote_monitor/), as the Callback used to stream events to a server  
* [tf.keras.callbacks.RemoteMonitor](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/RemoteMonitor), in TensorFlow documentation  


Examples of data, in scikit-learn (sklearn), Keras, etc:  

* [Real world datasets, in sklearn](https://scikit-learn.org/stable/datasets/real_world.html)  
* [California Housing price regression dataset, in Keras](https://keras.io/api/datasets/california_housing/)  
* [fetch_california_housing, in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), to load the California housing dataset  
* [astroNN](https://astronn.readthedocs.io/en/stable/) is a python package to do various kinds of neural networks with targeted application in astronomy by using Keras API  
* [Galaxy10 SDSS Dataset, in astroNN](https://astronn.readthedocs.io/en/latest/galaxy10sdss.html) 
* [Download Galaxy10 SDSS](https://astronn.readthedocs.io/en/latest/galaxy10sdss.html#download-galaxy10-sdss) 


Metrics, in scikit-learn (sklearn) and Keras:  

* list of [Metrics, in Keras API](https://keras.io/api/metrics/), contain functions that are used to judge the performance of your model  
* list of [Regression metrics, in Keras](https://keras.io/api/metrics/regression_metrics/), contain classes: MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredLogarithmicError, CosineSimilarity, LogCoshError  
* [tf.keras.metrics.R2Score](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/R2Score), in TensorFlow documentation  
* [r2_score, in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score), for computing the coefficient of determination  


Loss Function, in Keras:  

* list of [Losses, in Keras API](https://keras.io/api/losses/)  
* list of [Probabilistic losses, in Keras API](https://keras.io/api/losses/probabilistic_losses/)  
* list of [Regression losses, in Keras API](https://keras.io/api/losses/regression_losses/)  


Cross Entropy:  

* [Cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy), on wikipedia  
* [Kreuzentropie](https://de.wikipedia.org/wiki/Kreuzentropie), on german wikipedia  
* [One-Hot Encoding for Machine Learning with TensorFlow 2.0 and Keras](https://github.com/christianversloot/machine-learning-articles/blob/main/one-hot-encoding-for-machine-learning-with-tensorflow-and-keras.md), on github  


Calibration curve / Lernkurven, in scikit-learn (sklearn):  

* [Probability calibration, in sklearn](https://scikit-learn.org/stable/modules/calibration.html)  


Permutation feature importance, in scikit-learn (sklearn):  

* [Permutation feature importance, in sklearn](https://scikit-learn.org/stable/modules/permutation_importance.html)  
* [permutation_importance, in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html), as Permutation importance for feature evaluation  


fourth day discussion links:  

* [Image Classification on CIFAR-10](https://paperswithcode.com/sota/image-classification-on-cifar-10), for benchmarking  
* [Gemini, from Google DeepMind](https://deepmind.google/technologies/gemini/)  
* [Hands-on with Gemini](https://deepmind.google/technologies/gemini/#hands-on)  
* [Andrew Ng Criticizes the Culture of Overfitting in Machine Learning](https://www.unite.ai/andrew-ng-criticizes-the-culture-of-overfitting-in-machine-learning/)  


Handling Overfitting:  

* [Layer weight regularizers, in Keras](https://keras.io/api/layers/regularizers/), for implementing L1 & L2 Regularization  
* [tf.keras.regularizers.Regularizer](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer), in TensorFlow documentation  
* [Dropout layer, in Keras](https://keras.io/api/layers/regularization_layers/dropout/)  
* [EarlyStopping, in Keras](https://keras.io/api/callbacks/early_stopping/)  

```
pip install keras-tuner  
```


Optimization/Optimierung Übung/Exercise: Drop Out  

* [Drop Out, slides](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Deep_Learning/Woche_1/Drop_out.pdf), in PDF, on github  
* [Drop Out, slides](https://bitbucket.org/iscab/alfatraining_2023_deep_learning/src/master/Woche_1/Drop_out.pdf), in PDF, on bitbucket  
* google docs link?  


fifth day discussion links:  

* [Never use restore_best_weights=True with EarlyStopping](https://medium.com/@doleron/never-use-restore-best-weights-true-with-earlystopping-754ba5f9b0c6)  
* [hyperbolicfitdll, on github](https://github.com/bschulz81/hyperbolicfitdll), as an open source library that can help to autofocus telescopes  
* [robustregression, on github](https://github.com/bschulz81/robustregression), as a c++ library with statistical machine learning algorithms for linear and non-linear robust regression  


Python quiz, for LinkedIn:  

* [Python quiz](https://www.w3schools.com/python/python_quiz.asp), on w3schools  


Gradient Descent: Vanilla, Stochastic, and Mini Batch  

* [Differences Between Gradient, Stochastic and Mini Batch Gradient Descent](https://www.baeldung.com/cs/gradient-stochastic-and-mini-batch)  
* [Difference between Batch Gradient Descent and Stochastic Gradient Descent](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/)  
* [Difference Between SGD, GD, and Mini-batch GD](https://www.tutorialspoint.com/difference-between-sgd-gd-and-mini-batch-gd)  


### Project: EEG Data processing and classification (CANCELED)  

This topic were not chosen because the data loading was tricky and also my data was credential. 


Useful links for project:  

* [MNE in Python](https://github.com/mne-tools/mne-python), for MEG and EEG analysis and visualization  
* [MNE website](https://mne.tools/stable/index.html#)  
* [MNE dev](https://mne.tools/dev/index.html)  
* [MNE dev installation guide](https://mne.tools/dev/install/manual_install.html#manual-install)  
* [MNE installation guide](https://mne.tools/0.23/install/index.html)  
* [example from my MATLAB/EEGLAB code](https://bitbucket.org/iscab/bci_naovibe_footrace/src/master/EEG-analysis-script/scripts/ana01_filt_coaNAO.m)  


### Project: Image Super Resolution  

This topic is chosen because of data availability. 

Hints:  

* Super-Resolution  
* ResNet: Residual Network  
* Skip-connection  
* Batch Normalization  
* Peak signal-to-noise ratio (PSNR)  
* Structural similarity index measure (SSIM)  
* Image Quality metrics  
* Matplotlib  


drizzle algorithm:  

* suggestion "in astronomy one commonly uses a drizzle algorithm, perhaps you want to put that in as some layer?"  
* [Linear Reconstruction of the Hubble Deep Field](https://www.stsci.edu/~fruchter/dither/drizzle.html), explaining drizzle algorithm  
* suggestion "i suspect a working approach would be to apply drizzle 4x to enlarge the picture linearly and then apply a neuronal network, perhaps with convolutions in otder sharpen the image and make it smaller. the result is then a 2x larger image which is sharp"  


check:  

* [alfatraining](https://www.alfatraining.de/gefoerderte-weiterbildung/) courses  
* [alfatraining](https://www.alfatraining.de/) website  
* detailed in [my private repository](https://bitbucket.org/iscab/alfatraining_2023_deep_learning/)  
* [this deep learning notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Deep_Learning/my_notes/notes.md)  
* [this deep learning notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_deep_learning/src/master/my_notes/notes.md)  
* [other machine learning notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Machine_Learning/my_notes/notes.md)  
* [other machine learning notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_machine_learning/src/master/my_notes/notes.md)  
* [other python notes on github](https://github.com/iscab/belajar_python/blob/main/Course2023_alfatraining_Python_Programmierung/my_notes/notes.md)  
* [other python notes on bitbucket](https://bitbucket.org/iscab/alfatraining_2023_python/src/master/my_notes/notes.md)  


version: 08:00 09.02.2024

# End of File

